# ~/projects/RatesTheory
#+TITLE:Rates, Standardisation and Adjustment
#+AUTHOR: Ivan Hanigan
#+email: ivan.hanigan@anu.edu.au
#+LaTeX_CLASS: article
#+LaTeX_CLASS_OPTIONS: [a4paper]
#+LATEX: \tableofcontents
-----
#+name:load.project
#+begin_src R :session *R* :tangle no :exports none :eval no
  ###########################################################################
  # newnode: load.project
  require(ProjectTemplate)
  load.project()
#+end_src
* COMMENT TODO-list
** TODO get from /home/ivan/Dropbox/projects/IvanPhD/Papers/RatesAndStandardisation
** TODO get from keynote SoftwareSupportAndTraining/PostGIS@NCEPH/PostGISatNCEPH/Standardisation/standardisation.r/
** TODO paraphrase emails

Data Manager work FW: Spatial Statistics Advice
06 September 2013 09:48Ivan Hanigan
To:
 Geoff Mercer 
You replied on 06/09/2013 10:16.

HI Geoff,
FYI, just too keep you in the loop with what I have been doing (spdep is a forum I use to support spatial data analysis users, leveraging off spatial stats expertise in other ANU groups).
I chatted to Aparna about this and she knows the theory background here too, but I didn't feel right sending Elvina over to her for stats advice just yet.  Do you think that is appropriate?  I checked with Paul and Michael at APHCRI and they don't know this area, Rosemary Korda knows some of it but not in the mapping context, more in the Indirect SMRs original purpose – to compare the mortality from different causes within a single population.
________________________________________
From: Ivan Hanigan [Ivan.Hanigan@anu.edu.au]
Sent: 06 September 2013 09:39
To: Elvina Viennet
Cc: spdep@alliance.anu.edu.au
Subject: RE: spatial stats coffee today?

Attachments:

Julious2001JPublicHealth.pdf<https://alliance.anu.edu.au/access/content/attachment/ea804bb9-cb00-49f0-9642-49158e050915/_anon_/06aa4987-c56f-42b9-b643-669d7e123ff3/Julious2001JPublicHealth.pdf>

Julious2001JPublicHealthErrata.pdf<https://alliance.anu.edu.au/access/content/attachment/ea804bb9-cb00-49f0-9642-49158e050915/_anon_/375c4c71-64a3-47ce-8684-b58a6b0f0adc/Julious2001JPublicHealthErrata.pdf>

Note: The above attachments are stored on the Alliance server.
----------------------

HI EV, yes don't get bogged down on this.
But please don't make your decision before reading Julious attached - also with Errata (which I think showed they understated their point, "although the CMF still has bigger standard errors compared with the [Indirect] standardized mortality ratio (SMR), the differences are not so marked").
My position is that you should use Indirect because it is widely accepted to be appropriate in such rare diseases as you have (not literally) - but be aware that there is a deeper controversy in the field (conspiracy even? Note that Neither Anselin or the latest paper you sent mention Julious (nor Yule 1934) point that the Indirect SMR is not always a valid number to use in disease rate maps. Also recall Prof Bob Haining didn't even mention Direct was an option but just described Indirect as "the method".

I personally now have decided if I ever review a paper that uses the Indirect method they better jolly well have proved they checked their assumptions (but that is always the way reviewers operate right?)

Cheers,
I

________________________________________
From: Elvina Viennet
Sent: 06 September 2013 08:45
To: Ivan Hanigan
Subject: RE: spatial stats coffee today?

Thanks Ivan.

I have to make my decision .
Some more readings for myself, thought I should forward.

Cheers,
Elvina

From: Ivan Hanigan [mailto:Ivan.Hanigan@anu.edu.au]
Sent: Thursday, 5 September 2013 10:37 PM
To: spdep@alliance.anu.edu.au
Subject: FW: spatial stats coffee today?

Hi Francis,
Rate instability is a problem and rate shrinkage (empirical bayes or other) are appropriate solutions (Anselin and the SAL are field leaders here).

But...
Validity of the assumptions underlying the rate estimations in the first place are a crucial and often under-scrutinised aspect.

BUT
I've been pondering Elvina's next steps and feel that we may be "sweating the details" a bit too much here. If the aim is purely to get a publishable (defensible) descriptive map of disease rates then the minimally acceptable solution I think would be Indirectly age standardised SMRs using an internal standard (perhaps with some judicous shrinkage... possibly E-Bayes as you suggest). Given that hardly anybody ever checks that the assumption of proportional hazards holds for this method you should get away with it.

HTH -- I
________________________________
From: Francis Markham [francis.markham@anu.edu.au]
Sent: 05 September 2013 20:54
To: Ivan Hanigan
Cc: Elvina Viennet; spdep@alliance.anu.edu.au<mailto:spdep@alliance.anu.edu.au>
Subject: Re: spatial stats coffee today?
Sorry I missed you all today. The problem with rate mapping, as I understand it, is that in zones with small samples you get outliers (small number problem), especially for rare diseases. I had to do this once for visualisation (not analysis thankfully) and used a local empirical Bayes to smooth out some of this random variation, but I wasn't very happy with the result.

What do you think you'll end up doing? I'd be keen to learn what to do here in future...

Francis Markham
PhD candidate, Fenner School of Environment and Society
Research Officer, Centre for Aboriginal Economic Policy Research
Australian National University
Ph: +61-488-196-318
http://fennerschool.anu.edu.au/about-us/people/francis-markham

On 5 September 2013 20:45, Ivan Hanigan <Ivan.Hanigan@anu.edu.au<mailto:Ivan.Hanigan@anu.edu.au>> wrote:
Thanks Elvina,
Whilst we discussed Direct Standardised Mortality/Morbidity Rates and Ratios (SMRs) at length today, this method is not suitable for the case you are dealing with, namely rare diseases in very small areas.

Unfortunately while the Indirect SMR method is often used in this case, it is noted by Anselin et al (and many others) that

"indirectly adjusted rates tend not to be comparable across areas."

So mapping these is problematic.

The main limitation of indirect standardisation is that it assumes proportional hazards for the standard rates compared with the observed rates. Is this true? If so, then you can use indirect standardisation to compare regions on the same map. If not then this is invalid.

From http://www.statsdirect.com/webhelp/#rates/smr.htm
"Indirect SMRs from different index/study populations are not strictly comparable because they are calculated using different weighting schemes that depend upon the age structures of the index/study populations."

So how can we check the proportional hazards assumption for the standard rates compared with the observed rates?

Indirect SMRs can be compared if you make the assumption that the ratio of rates between index and reference populations is constant; this is similar to the assumption of proportional hazards in Cox regression (Armitage and Berry, 1994).

SO If we were to calculate the Annualised Age Specific Rates for our study areas and for our standard for several years at periodic timepoints across the study period, and then calculate the ratio of these at each timepoint we could reassure our selves that this assumption holds.

At the moment after some experimenting with Pysal I prefer the R implementations (such as http://medepi.com/epitools/ see the ageadjust.indirect() function.)

The other option is to aggregate to larger units but I fear your particular disease is so rare that this will not solve the issue.

ALl the best,
Ivan.
________________________________________
From: Elvina Viennet [Elvina.Viennet@anu.edu.au<mailto:Elvina.Viennet@anu.edu.au>]
Sent: 05 September 2013 15:50
To: spdep@alliance.anu.edu.au<mailto:spdep@alliance.anu.edu.au>
Subject: RE: spatial stats coffee today?
Attachments:

2006_ANSELIN_Rate transformations and smoothing.pdf<https://alliance.anu.edu.au/access/content/attachment/ea804bb9-cb00-49f0-9642-49158e050915/_anon_/251acd42-83b4-4691-92d7-b7d8a6328e8b/2006_ANSELIN_Rate%20transformations%20and%20smoothing.pdf><https://alliance.anu.edu.au/access/content/attachment/ea804bb9-cb00-49f0-9642-49158e050915/_anon_/251acd42-83b4-4691-92d7-b7d8a6328e8b/2006_ANSELIN_Rate%20transformations%20and%20smoothing.pdf%3E>

Note: The above attachments are stored on the Alliance server.
----------------------
Following our really interesting talk, here is another paper.
Thanks again.

Cheers,
E

From: Ivan Hanigan [mailto:Ivan.Hanigan@anu.edu.au]
Sent: Thursday, 5 September 2013 10:34 AM
To: spdep@alliance.anu.edu.au<mailto:spdep@alliance.anu.edu.au>
Subject: spatial stats coffee today?

any topics out there?
Elvina and I have been discussing disease rate mapping and I've got a rant brewing about python, specifically this silly implementation of a 'bread-and-butter' method for epidemiology and demography.

1 Crude Age Standardization¶

In this approach, the rate for an area is simply the sum of age-specific rates weighted by the ratios of each age group in the total population.

To obtain the rates based on this approach, we first need to create two variables that correspond to event counts and population values, respectively.

1.1 crude-pysal-code

The following is paraphrased from http://pythonhosted.org/PySAL/users/tutorials/smoothing.html#age-standardization-in-pysal

To apply the crude age standardization in pysal, we need to make the following function call (after first creating some test data):

import numpy as np

from pysal.esda import smoothing as sm

e = np.array([30, 25, 25, 15, 33, 21, 30, 20])

b = np.array([100, 100, 110, 90, 100, 90, 110, 90])

sm.crude_age_standardization(e, b, 2)

In the function call above, the last argument indicates the number of area units.
array([ 0.2375 , 0.26666667])


The outcome in the second line shows that the age-standardized rates for two areas are about 0.24 and 0.27, respectively.

Each set of numbers should include n by h elements where n and h are the number of areal units and the number of age groups. In the above example there are two regions with 4 age groups. Age groups are identical across regions. The first four elements in b represent the populations of 4 age groups in the first region, and the last four elements the populations of the same age groups in the second region.

1.2 Critique
-* The requirement to specify number of areas is silly (area-code is a common dimension of such datasets)
-* Does this take matrices or pandas dataframes? Health data usually are structured in tables.

1.3 crude-R-code

The same thing is trivial in R. But this time we'll take a data.frame of cases and populations from two areas rather than seperate vectors, which is a more common way for health data to be arranged.

# func

require(plyr)

# load

df <- read.table(textConnection(

"a e b

a 30 100

a 25 100

a 25 110

a 15 90

b 33 100

b 21 90

b 30 110

b 20 90"

), sep = "", header = T)

# check

str(df)

# do

ddply(df, 'a', summarise, counts = sum(e), pop = sum(b), rate = sum(e)/sum(b))




________________________________

This automatic notification message was sent by Alliance (https://alliance.anu.edu.au/portal) from the spdep site.
You can modify how you receive notifications at My Workspace > Preferences.

[see attachment: "2006_ANSELIN_Rate transformations and smoothing.pdf", size: 376818 bytes]

________________________________

This automatic notification message was sent by Alliance (https://alliance.anu.edu.au/portal) from the spdep site.
You can modify how you receive notifications at My Workspace > Preferences.
________________________________

This automatic notification message was sent by Alliance (https://alliance.anu.edu.au/portal) from the spdep site.
You can modify how you receive notifications at My Workspace > Preferences.


________________________________

This automatic notification message was sent by Alliance (https://alliance.anu.edu.au/portal) from the spdep site.
You can modify how you receive notifications at My Workspace > Preferences.[see attachment: "Julious2001JPublicHealth.pdf", size: 78043 bytes]

[see attachment: "Julious2001JPublicHealthErrata.pdf", size: 29760 bytes]


________________________________

This automatic notification message was sent by Alliance (https://alliance.anu.edu.au/portal) from the spdep site.
You can modify how you receive notifications at My Workspace > Preferences.




* COMMENT init
** init
#+name:conf
#+begin_src text :tangle config/global.dcf :exports none :eval no
data_loading: on
cache_loading: on
munging: on
logging: off
load_libraries: off
libraries: reshape, plyr, ggplot2, stringr, lubridate, epitools, foreign
as_factors: on
data_tables: off
#+end_src

#+name:init
#+begin_src R :session *shell* :tangle init.r :exports none :eval no
  ###########################################################################
  # newnode: init
  if (!require(reshape)) install.packages('reshape', repos='http://cran.csiro.au'); require(reshape)
  if (!require(plyr)) install.packages('plyr', repos='http://cran.csiro.au'); require(plyr)
  if (!require(ggplot2)) install.packages('ggplot2', repos='http://cran.csiro.au'); require(ggplot2)
  if (!require(stringr)) install.packages('stringr', repos='http://cran.csiro.au'); require(stringr)
  if (!require(lubridate)) install.packages('lubridate', repos='http://cran.csiro.au'); require(lubridate)
  if (!require(epitools)) install.packages('epitools', repos='http://cran.csiro.au'); require(epitools)
  if (!require(foreign)) install.packages('foreign', repos='http://cran.csiro.au'); require(foreign)
  
#+end_src

** Additions
#+name:additions
#+begin_src R :session *R* :tangle init.r :exports none :eval no
  ####
  # init additional directories for project management
  source('~/Dropbox/tools/analysisTemplate.r')
  analysisTemplate()
#+end_src
* Introduction to rates, standardisation and adjustment

The aim of this project is to explore the methodological issues of rates, standardisation and adjustment in regression models.
This topic is relevant in epidemiology and demography (and most human sciences).

The demonstration will utilize the R codes and data from https://github.com/ivanhanigan/RatesTheory

* Theoretical background
Different age or sex structures of study populations can be the reason for important differences between the health outcomes of the populations that need to be accounted for when assessing other putative causal relationships such as socio-economic status or the physical environment. 

There are two common methods for age standardisation: direct and indirect. There is also an alternative method using Poisson models that adjust for age as a covariate.
* Compared to what?
Question: Hi Epidemiologist, how are you?

Epidemiologist: Compared to what?
* Standardisation strengths and weaknesses
The direct and indirect methods have a long history of use to control for age (and indeed other differences between populations such as sex and other stratifying variables).  However, the idea that indirect standardisation is based on the non-study population as the common standard is a misconception which results in a common methodological error: comparing indirectly age standardised incidence ratios (these can only really compare the study population to the standard population, not between study populations). 

While direct standardization does provide comparable measures it has other weaknesses, such as greater susceptibility than the indirect method to error with small numbers. Indirect incidence ratios can be compared if you make the assumption that the ratio of rates between the study and standard populations is constant; this is similar to the assumption of proportional hazards in Cox regression. 
* Adjustment in regression
The aim of standardisation is to control for a compositional variable. This, of course, is also one of the main aims of regression analysis. One could analyse the data using Poisson regression, a method appropriate for small numbers. An advantage of the regression approach is that one can easily control for multiple confounders. Also, we can test for the presence of an interaction, which would question the validity of the additive model underlying direct standardization. 
* Data

** get the princeton tutorial
The website by German Rodriguez from Princeton is good [[http://data.princeton.edu/eco572/std.html]]
Has some data and methods, comparing with book Demography: measuring and modeling population processes? Samuel H. Preston, Patrick Heuveline, Michel Guillot - 2001.
get data from [[http://data.princeton.edu/eco572/datasets/preston21long.dat]]
on 13-4-12

#+name:load-princeton-tute
#+begin_src R :session *R* :tangle src/load-princeton-tute.r :exports reports :eval no
  ###########################################################################
  # newnode: load-princeton-tute
  
    # dl
    download.file('http://data.princeton.edu/eco572/datasets/preston21long.dat', destfile = 'data/preston21long.dat', mode = 'wb')
     # load
     d <- read.table('http://data.princeton.edu/eco572/datasets/preston21long.dat', col.names = c('country', 'ageg', 'pop', 'deaths'))
     write.csv(d, 'data/preston21long.csv', row.names = F)
     
     # check
     head(d)
     png('reports/ageRates.png', res = 100)
     with(subset(d, country == 'Sweden'), plot((deaths/pop)*1000, log = 'y', type = 'l', col='blue'))
     with(subset(d, country == 'Kazakhstan'), lines((deaths/pop)*1000, col='red'))
     legend('bottomright', c('Kazakhstan','Sweden'), lty = 1, col = c('red','blue'))
     dev.off()
   
     
#+end_src
** get the stata tutorial
We will use data borrowed from Kahn and Sempos (1989, 95-105) that are available on the Stata website, and in the datasets and do-files subdirectory.  The problem is (Stata 9, Ref A-J, p. 310), We want to compare 1970 mortality rates in California and Maine, adjusting for age.  Although we have age-specific population counts for the two states, we lack age-specific death rates.  In this situation, direct standardization is not feasible.  We can use the US population census data for the same year to produce indirectly standardized rates for the these two states.       
downloaded 13-4-12

#+name:stata tute
#+begin_src R :session *R* :tangle main.R :exports reports :eval no
  # dl
  #popkahn <- read.dta('http://www.stata-press.com/data/r9/popkahn.dta')
  #popkahn        
          
  #kahn <- read.dta('http://www.stata-press.com/data/r9/kahn.dta')
  #kahn
  
    download.file('http://www.stata-press.com/data/r9/popkahn.dta', destfile = 'data/popkahn.dta', mode = 'wb')
  
    download.file('http://www.stata-press.com/data/r9/kahn.dta', destfile = 'data/kahn.dta', mode = 'wb')
#+end_src

* Analysis
\section{Analysis}
** available tools
*** epitools
#+name:do-epitools
#+begin_src R :session *R* :tangle src/do-epitools.r :exports reports :eval no
#######################################################################
# name: do-epitools
# epitools has direct and indirect functions
# TODO stataCompare
 
##From Selvin (2004)
##enter data
dth60 <- as.numeric(read.table(textConnection('141 926 1253 1080 1869 4891 14956 30888 41725 26501 5928')))
pop60 <- as.numeric(read.table(textConnection('1784033 7065148 15658730 10482916 9939972 10563872 9114202 6850263 4702482 1874619 330915')))
dth40 <- as.numeric(read.table(textConnection('45 201 320 670 1126 3160 9723 17935 22179 13461 2238')))
pop40 <- as.numeric(read.table(textConnection('906897 3794573 10003544 10629526 9465330 8249558 7294330
5022499 2920220 1019504 142532')))
##calculate age-specific rates
rate60 <- dth60/pop60
rate40 <- dth40/pop40
#create array for display
tab <- array(c(dth60, pop60, round(rate60*100000,1), dth40, pop40,
round(rate40*100000,1)),c(11,3,2))
agelabs <- c('<1', '1-4', '5-14', '15-24', '25-34', '35-44', '45-54',
'55-64', '65-74', '75-84', '85+')
dimnames(tab) <- list(agelabs,c('Deaths', 'Population', 'Rate'),
c('1960', '1940'))
tab
##implement direct age standardization using ’ageadjust.direct’
dsr <- ageadjust.direct(count = dth40, pop = pop40, stdpop = pop60)
round(100000*dsr, 2) ##rate per 100,000 per year
##implement indirect age standardization using ’ageadjust.indirect’
isr <- ageadjust.indirect(count = dth40, pop = pop40,
stdcount = dth60, stdpop = pop60)
round(isr$sir, 2) ##standarized incidence ratio
round(100000*isr$rate, 1) ##rate per 100,000 per year 
  
#+end_src

* Age Specific Rates
* Crude Age Standardization
In this approach, the rate for an area is simply the sum of age-specific rates weighted by the ratios of each age group in the total population.

To obtain the rates based on this approach, we first need to create two variables that correspond to event counts and population values, respectively.
*** crude-pysal-code
The following is paraphrased from http://pythonhosted.org/PySAL/users/tutorials/smoothing.html#age-standardization-in-pysal

To apply the crude age standardization in pysal, we need to make the following function call (after first creating some test data):

#+name:crude-pysal
#+begin_src python :session *shell* :tangle pysal-crd.py :eval no
  import numpy as np
  from pysal.esda import smoothing as sm
  e = np.array([30, 25, 25, 15, 33, 21, 30, 20])
  b = np.array([100, 100, 110, 90, 100, 90, 110, 90])  
  out = sm.crude_age_standardization(e, b, 2)
  print out
#+end_src

*** pysal-crd-run-code
#+name:pysal-crd-run
#+begin_src sh :session *shell* 
python pysal-crd.py
#+end_src

#+RESULTS: pysal-crd-run
: [ 0.2375      0.26666667]



In the function call above, the last argument indicates the number of area units. 

|array([ 0.2375    ,  0.26666667])|

The outcome in the second line shows that the age-standardized rates for two areas are about 0.24 and 0.27, respectively.

Each set of numbers should include n by h elements where n and h are the number of areal units and the number of age groups. In the above example there are two regions with 4 age groups. Age groups are identical across regions. The first four elements in b represent the populations of 4 age groups in the first region, and the last four elements the populations of the same age groups in the second region.

*** Critique
- The requirement to specify number of areas is silly (area-code is a common dimension of such datasets)
- Does this take matrices or pandas dataframes?  Health data usually are structured in tables.
*** COMMENT crude-R-create-df-code
#+name:crude
#+begin_src R :session *R* :tangle no :eval no
  ################################################################
  # name:crude
  # aim R implement 
  e =c(30, 25, 25, 15, 33, 21, 30, 20)
  b = c(100, 100, 110, 90, 100, 90, 110, 90)
  a = c(rep("a",4),rep("b",4))
  df <- data.frame(a,e,b)
#+end_src
*** crude-R-code
The same thing is trivial in R.  But this time we'll take a data.frame of cases and populations from two areas rather than seperate vectors, which is a more common way for health data to be arranged. 

#+name:crude-R
#+begin_src R :session *R* :tangle no :eval yes
  # func
  require(plyr)
  # load
  df <- read.table(textConnection(
  "a  e   b
  a 30 100
  a 25 100
  a 25 110
  a 15  90
  b 33 100
  b 21  90
  b 30 110
  b 20  90"
  ), sep = "", header = T)
  # check
  str(df)
  # do
  ddply(df, 'a', summarise, counts = sum(e), pop = sum(b), rate = sum(e)/sum(b))
  
#+end_src

#+RESULTS: crude-R
| a |  95 | 400 |            0.2375 |
| b | 104 | 390 | 0.266666666666667 |

* Direct Age standardisation
** dstdize
#+name:do-dstdize
#+begin_src R :session *R* :tangle src/do-dstdize.r :exports reports :eval no
  #######################################################################
  # name: do-dstdize
  # studypops        
  d <- read.table('http://data.princeton.edu/eco572/datasets/preston21long.dat', col.names = c('country', 'ageg', 'pop', 'deaths'))
  head(d)
   
  # standard
  standard<- ddply(d, 'ageg', function(df) return(c(pop=sum(df$pop))))
  
  # epitools needs single
  do <- subset(d, country == 'Sweden')   # Kazakhstan
  ageadjust.direct(count=do$deaths, pop=do$pop, stdpop=standard$pop)     
          
  rageadjust.direct <- function (data, count, pop, rate = NULL, stdpop, by, using = NA,print=T, time = NULL, conf.level = 0.95, age = 'age'){
  
  if (!require(plyr)) install.packages('plyr', repos='http://cran.csiro.au'); require(plyr)
  d <- data
  studysite <- by
  standard <- using
  agevar <- age
  
  if (missing(count) == TRUE & !missing(pop) == TRUE & is.null(rate) == TRUE) {
  d$count <- d[,rate] * d[,pop]
  }
  if (missing(pop) == TRUE & !missing(count) == TRUE & is.null(rate) == TRUE) {
  d$pop <- d[,count]/d[,rate]
  }
  if (is.null(rate) == TRUE & !missing(count) == TRUE & !missing(pop) == TRUE) {
  d$rate <- d[,count]/d[,pop]
  }
  alpha <- 1 - conf.level
  
  if(is.null(time)){
          observed<-ddply(d, c(studysite), function(df) return(c(observed = sum(df[,count]), pop = sum(df[,pop]), crude.rate = sum(df[,count])/sum(df[,pop])))) 
          standard$stdwt <- standard[,stdpop]/sum(standard[,stdpop])
          d<- merge(d,standard, by = age) 
          dsr <- ddply(d, by, function(df) return(c(dsr = sum(df$stdwt * df$rate))))
          names(d) <- gsub(paste(pop,'.x',sep=''), pop, names(d))
          dsr.var <- ddply(d, by, function(df) return(c(dsr.var = sum((df$stdwt^2) * (df[,count]/df[,pop]^2))))) 
          wm <- ddply(d, by, function(df) return(c(wm=max(df$stdwt/df[,pop]))))
          dsr<-merge(dsr, dsr.var, by = by)
          dsr<-merge(dsr, wm, by = by)
  
          gamma.lci <- ddply(dsr, by, function(df) 
                  return(c(lci=qgamma(alpha/2, shape = (df$dsr^2)/df$dsr.var, scale = df$dsr.var/df$dsr)
                  )))
          gamma.uci <- ddply(dsr, by, function(df) 
                  return(c(uci=qgamma(1 - alpha/2, shape = ((df$dsr + df$wm)^2)/(df$dsr.var + df$wm^2), scale = (df$dsr.var + df$wm^2)/(df$dsr + df$wm))
                  )))
          dsr<-merge(dsr, gamma.lci, by = by)
          dsr<-merge(dsr, gamma.uci, by = by)
          names(dsr) <- gsub('dsr', 'adj.rate', names(dsr)) 
          outdat <- merge(observed,dsr[,c('country','adj.rate','lci','uci')])
  } else {
  observed<-ddply(d, c(studysite, time), function(df) return(c(observed = sum(df[,count]), pop = sum(df[,pop]), crude.rate = sum(df[,count])/sum(df[,pop])))) 
  standard$stdwt <- standard[,stdpop]/sum(standard[,stdpop])
  d<- merge(d,standard, by = age) 
  dsr <- ddply(d, c(by, time), function(df) return(c(dsr = sum(df$stdwt * df$rate))))
  names(d) <- gsub(paste(pop,'.x',sep=''), pop, names(d))
  dsr.var <- ddply(d, c(by, time), function(df) return(c(dsr.var = sum((df$stdwt^2) * (df[,count]/df[,pop]^2))))) 
  wm <- ddply(d, c(by, time), function(df) return(c(wm=max(df$stdwt/df[,pop]))))
  dsr<-merge(dsr, dsr.var, by = c(by, time))
  dsr<-merge(dsr, wm, by = c(by, time))
  
  gamma.lci <- ddply(dsr, c(by, time), function(df) 
          return(c(lci=qgamma(alpha/2, shape = (df$dsr^2)/df$dsr.var, scale = df$dsr.var/df$dsr)
          )))
  gamma.uci <- ddply(dsr, c(by, time), function(df) 
          return(c(uci=qgamma(1 - alpha/2, shape = ((df$dsr + df$wm)^2)/(df$dsr.var + df$wm^2), scale = (df$dsr.var + df$wm^2)/(df$dsr + df$wm))
          )))
  dsr<-merge(dsr, gamma.lci, by = c(by, time))
  dsr<-merge(dsr, gamma.uci, by = c(by, time))
  names(dsr) <- gsub('dsr', 'adj.rate', names(dsr)) 
  outdat <- merge(observed,dsr[,c(by, time,'adj.rate','lci','uci')])
  
  }
  return(outdat)          
  }
  
  rageadjust.direct(data = d, age ='ageg', count='deaths', pop='pop', stdpop='pop', using=standard, by = 'country')     
  
  d$day <- c(rep(1,19),rep(2,19))
  d$studysite <- 'allTheSame'
  rageadjust.direct(data = d, age ='ageg', count='deaths', pop='pop', stdpop='pop', using=standard, by = 'studysite', time = 'day')     
  
#+end_src

** directRates
*** func
**** func-directRates


#+name:func-directRates.r
#+begin_src R :session *R* :tangle src/func-directRates.r :exports reports :eval no 


directRates <- function(analyte, standard_pop, stratify.var = c('dthdate')){       
 #  analyte = time series of outcomes abd populations, by age and sex
 #  standard_pop = standard
 # stratify.var = c('dthdate','sex') # by sex if wanted age rates for each sex, could also be by zone?
 # TODO 
 #  make this work with multiple study populations?
 # if study_pop = NA then will check if multiple study zones, will use the total population, if by time then will use mid point?
 if(!require(plyr)) install.packages('plyr',repos='http://cran.csiro.au'); require(plyr)

 # step 1 get the standard population
 # TODO generalise to the optional inclusion of a standard

 # step 2 for each time step calc the age specific rates in study, apply to standard pops
 # need to merge        
 analyte <- merge(analyte, standard_pop, all.x = T) #, by.x= 'age', by.y ='age')
 
 # get the daily age specific rates of the ROS and apply to standard
 # this is the expected number of deaths if the standard had had the same health experience as the study
 analyte$allcause_asr <- (analyte$allcause/analyte$pop) * analyte$standard_pop
 analyte$resp_asr <- (analyte$resp/analyte$pop) * analyte$standard_pop
 analyte$cvd_asr <- (analyte$cvd/analyte$pop) * analyte$standard_pop

        
 # step 3 sum expected deaths over age, stratify by stratify.var      
 dailystandard <- ddply(analyte, stratify.var, function(df) return(c(
  standard_pop_summed = sum(df$standard_pop),
  allcause_asr_summed = sum(df$allcause_asr),
  resp_asr_summed = sum(df$resp_asr), 
  cvd_asr_summed = sum(df$cvd_asr))))

 # and divide by standard population x 100,000 
 dailystandard$allcause_stndrate <- (dailystandard$allcause_asr_summed/dailystandard$standard_pop_summed) * 100000
 dailystandard$resp_stndrate <- (dailystandard$resp_asr_summed/dailystandard$standard_pop_summed) * 100000
 dailystandard$cvd_stndrate <- (dailystandard$cvd_asr_summed/dailystandard$standard_pop_summed) * 100000

 return(dailystandard)
 }

#+end_src

*** COMMENT TODO load
*** COMMENT TODO clean
*** COMMENT TODO do

** direct-pysal-code
Direct age standardization is a variation of the crude age standardization. While crude age standardization uses the ratios of each age group in the observed population, direct age standardization weights age-specific rates by the ratios of each age group in a reference population. This reference population, the so-called standard million, is another required argument in the PySAL implementation of direct age standardization:

#+name:direct-pysal
#+begin_src python :session *shell* :tangle direct-pysal.py :exports reports :eval no
s = np.array([100, 90, 100, 90, 100, 90, 100, 90])
rate = sm.direct_age_standardization(e, b, s, 2, alpha=0.05)
np.array(rate).round(6)
#+end_src

Results:

#+begin_src python :session *shell* :tangle no :exports reports :eval no
array([[ 0.23744 ,  0.192049,  0.290485],
       [ 0.266507,  0.217714,  0.323051]])
#+end_src

The outcome of direct age standardization includes a set of standardized rates and their confidence intervals. The confidence intervals can vary according to the value for the last argument, alpha
** TODO direct-r-vs-pysal

* Indirect Age standardisation
** istdize
We will use data borrowed from Kahn and Sempos (1989, 95-105) that are available on the Stata website, and in the datasets and do-files subdirectory.  The problem is (Stata 9, Ref A-J, p. 310), We want to compare 1970 mortality rates in California and Maine, adjusting for age.  Although we have age-specific population counts for the two states, we lack age-specific death rates.  In this situation, direct standardization is not feasible.  We can use the US population census data for the same year to produce indirectly standardized rates for the these two states.       

#+name:do-istdize
#+begin_src R :session *R* :tangle src/do-istdize.r :exports reports :eval no
#######################################################################
# name: do-istdize

popkahn <- read.dta('http://www.stata-press.com/data/r9/popkahn.dta')
popkahn        
        
kahn <- read.dta('http://www.stata-press.com/data/r9/kahn.dta')
kahn



#for(st in c('California', 'Maine')){
# st <- 'Maine'
# print(st)        
do <- subset(kahn, state == 'Maine')   
# note needs counts for each age, but Main only has death in first row
do$death <- do$death[1]        
print(ageadjust.indirect(count=do$death/length(do$death), pop=do$population, stdcount = popkahn$deaths, stdpop=popkahn$population))
#}

#+end_src

** rewrite with studypop and time
#+name:do-istdize-with-pop-and-time
#+begin_src R :session *R* :tangle src/do-istdize-with-pop-and-time.r :exports reports :eval no
#######################################################################
# name: do-istdize-with-pop-and-time
# rewrite with by studypop and time

rageadjust.indirect <- function (data, count, pop, using, stdcount, stdpop, stdrate = NULL, conf.level = 0.95, by, time = NULL){
	if (!require(plyr)) install.packages('plyr', repos='http://cran.csiro.au'); require(plyr)
	# count can either be age specific if known for study pops or a total deaths if unknown (in which case should be a fraction that sums to the total)
	d <- data
	studysite <- by
	standard <- using

	# if both have a col called death and population the combined names will have.x or .y so rename first
	names(standard) <- gsub(stdcount, paste(stdcount,'Std',sep=''), names(standard))
	names(standard) <- gsub(stdpop, paste(stdpop,'Std',sep=''), names(standard))
	d <- merge(d,standard, all.x=T, by = 'age')

	zv <- qnorm(0.5 * (1 + conf.level))

	if(is.null(time)){
		observed<-ddply(d, c(studysite), function(df) return(c(observed = sum(df[,count]), pop = sum(df[,pop]), crude.rate = sum(df[,count])/sum(df[,pop])))) 
		# NOT DONE YET
		# if (is.null(stdrate) == TRUE & length(stdcount) > 1 & length(stdpop > 
			# 1)) {
			# stdrate <- stdcount/stdpop
		# }
		expected <- ddply(d, c(studysite), function(df) return(c(stdcrate=sum(df[, paste(stdcount,'Std',sep='')])/sum(df[,paste(stdpop,'Std',sep='')]), expected = sum((df[, paste(stdcount,'Std',sep='')]/df[,paste(stdpop,'Std',sep='')]) * df[,pop])))) 
	} else {

		observed<-ddply(d, c(studysite, time), function(df) return(c(observed = sum(df[,count]), pop = sum(df[,pop]), crude.rate = sum(df[,count])/sum(df[,pop])))) 
		expected <- ddply(d, c(studysite, time), function(df) return(c(stdcrate=sum(df[, paste(stdcount,'Std',sep='')])/sum(df[,paste(stdpop,'Std',sep='')]), expected = sum((df[, paste(stdcount,'Std',sep='')]/df[,paste(stdpop,'Std',sep='')]) * df[,pop])))) 

	}

	outdat <- merge(observed, expected)
	outdat$sir <- outdat$observed/outdat$expected
	outdat$logsir.lci <- log(outdat$sir) - zv * (1/sqrt(outdat$observed))
	outdat$logsir.uci <- log(outdat$sir) + zv * (1/sqrt(outdat$observed))
	outdat$sir.lci <- exp(outdat$logsir.lci)
	outdat$sir.uci <- exp(outdat$logsir.uci)
	outdat$adj.rate <- outdat$sir * outdat$stdcrate
	outdat$adj.rate.lci <- outdat$sir.lci * outdat$stdcrate
	outdat$adj.rate.uci <- outdat$sir.uci * outdat$stdcrate
	if(is.null(time)){
	outdat <- outdat[,c(studysite,'observed','expected','sir','sir.lci','sir.uci','crude.rate','adj.rate','adj.rate.lci','adj.rate.uci')]
	} else {
	outdat <- outdat[,c(studysite,time,'observed','expected','sir','sir.lci','sir.uci','crude.rate','adj.rate','adj.rate.lci','adj.rate.uci')]
	}        
	return(outdat)
}


# standard
popkahn <- read.dta('http://www.stata-press.com/data/r9/popkahn.dta')
popkahn        
# studypops        
kahn <- read.dta('http://www.stata-press.com/data/r9/kahn.dta')
kahn
# note needs counts for each age, but Main only has death in first row     
kahn[kahn$state == 'Maine','death'] <- 11051
kahn
# need to create the fraction of deaths in the age groups for this example to work
kahn$count <- kahn$death/(length(kahn$death)/length(table(kahn$state)))

rageadjust.indirect(data=kahn, by = 'state', time = NULL, using = popkahn, count='count', pop='population', stdcount = 'deaths', stdpop='population')


# check orig
do <- subset(kahn, state == 'Maine')   

ageadjust.indirect(count=do$death/length(do$death), pop=do$population, stdcount = popkahn$deaths, stdpop=popkahn$population)

rage <- rageadjust.indirect(data=do, by = 'state', time = NULL, using = popkahn, count='count', pop='population', stdcount = 'deaths', stdpop='population')

as.data.frame(t(rage[1,]))


#+end_src


** indirect-pysal-code
While direct age standardization effectively addresses the variety in the risks across age groups, its indirect counterpart is better suited to handle the potential imprecision of age-specific rates due to the small population size. This method uses age-specific rates from the standard million instead of the observed population. It then weights the rates by the ratios of each age group in the observed population. To compute the age-specific rates from the standard million, the PySAL implementation of indirect age standardization requires another argument that contains the counts of the events occurred in the standard million.

#+name:indirect-pysal
#+begin_src python :session *shell* :tangle indirect-pysal.py :exports reports :eval no
s_e = np.array([10, 15, 12, 10, 5, 3, 20, 8])
rate = sm.indirect_age_standardization(e, b, s_e, s, 2, alpha=0.05)
np.array(rate).round(6)
#+end_src

Results

#+begin_src python :session *shell* :tangle no :exports reports :eval no
array([[ 0.208055,  0.170156,  0.254395],
       [ 0.298892,  0.246631,  0.362228]])
#+end_src

The outcome of indirect age standardization is the same as that of its direct counterpart.

** TODO indirect-r-vs-pysal

* Adjustment using regression
* Control for secular trend
* Uses in spatial epidemiology

* Indirect standardisation controlling for spatial correlation

- We'll use the example of the Conditional Autoregressive (CAR) model of Lip cancer in Scotland.
- Hierarchical Modeling and Analysis for Spatial Data (ISBN: 1-58488-410-X), by S. Banerjee, B.P. Carlin and A.E. Gelfand, Boca Raton, FL: Chapman and Hall/CRC Press, 2004. 
- Lipsbrad.odc, the full WinBUGS code for the Scottish lip cancer example (page 167) http://www.biostat.umn.edu/~brad/data2.html
* Regression approach to spatial rates
Mantel and Stark (1968), with reference to an alternative approach to indirect age standardisation. This is useful when the data are being internally standardised (using the data themselves as the standard) 
and where there is potential confounding. The general approach is to use a regression model with the variable to be standardised (eg age) and with the stratification variable which is potentially confounded (eg area). 
The standardised rates by the stratification variable can then be found from the regression predictions scaled to the observed total. Note that this approach requires non-zero cells for each stratum (eg at least one event per area).

- Other references: Breslow and Day (1975), Esteve et al. (1994, p90-92).
- see Mark's SAS implementation at keynote tools/Statistical Rules of Thumb/standardised incidence ratios/regression approach 574

* Weight by inverse of variance
** Weighted Regression
In regression analyses the age-standardized rates can be used as the
response variable and will probably suit a normal OLS or gaussian GLM.
In many cases weighted regression may be more appropriate, where each
point does not contribute the same amount of information to fitting
the regression line. It is common to use weights (see \cite{Armitage} and \cite{Boyle} (page 141).)
#+begin_src R 
wi = l/Var (yi)
#+end_src

** Calculate the Variance of an Areal Unit's Rate
To calculate the variance for each age standardised rate at each time
point, first the proportion of the age group in the standard
population (that is the population in that age group divided by the
total population in the standard multiplied by 1000) is applied to the
counts observed in that age group in the study population (Mark
Clements 2005 personal communication). Then this is divided by the
square of the population in that age group in the study area. This is
then summed for each area. Then the inverse is used as the weight in
the regression model.  Expressed as an equation this is:

#+begin_src R 
Variance (age standardised ratej) = (Σ W j * y ij )/ Σ Nij^2

Where:
j = area,
y = deaths,
i = age,
N = population, and
W j = (Population in age group i   /Total population in standard ) * 1000
#+end_src
*** Inverse variance weighting of direct rates
The use of direct rates is used with caution in small subpopulations because it is well-known to be prone to imprecision and noise. This can be addressed by comparing results of the unweighted regression with regression weighted by the inverse of the variances of the standardized rates. To calculate the variance for each neigborhood’s standardized rate, first the proportion of the age group in the standard population (that is the population in that age group divided by the total population in the standard multiplied by 1000) is applied to the counts observed in that age group in the study population. Then this is divided by the square of the population in that age group in the study area. This is then summed for each area. Then the inverse is used as the weight in the regression model.

Expressed as an equation this is: 
Variance (age standardised ratej) = Σ W i * y ij / Σ Nij2
Where j = area, y = deaths, i = age, N = population, and 
Wi = (Population in age group i / Total population in standard) * 1000 
These weights are used by the least-squares algorithm which minimizes the sum of the squared residuals multiplied by the weights.

* TODO References
1. P Armitage and G Berry. Statistical methods in medical research. Oxford: Blackwell Scientific, 1987.

2. P Boyle and D M Parkin. Chapter 11 . Statistical methods for registries. In International Agency for Research on Cancer. 1991.

* COMMENT ref
\bibliographystyle{unsrt}
\bibliography{/home/ivan/references/library}


